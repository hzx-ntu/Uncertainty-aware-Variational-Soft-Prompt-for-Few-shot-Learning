python3 scripts/train.py \
  --data 'data/downstream/ds002105' \
  --n-train-subjects-per-dataset $test_num \
  --n-val-subjects-per-dataset 3 \
  --n-test-subjects-per-dataset 9 \
  --architecture 'GPT' \
  --pretrained-model 'results/models/upstream/GPT_lrs-4_hds-12_embd-768_train-CSM_lr-0005_bs-192_drp-01/model_final/pytorch_model.bin' \
  --training-style 'decoding' \
  --decoding-target 'task_label.pyd' \
  --num-decoding-classes 26 \
  --training-steps 5000 \
  --per-device-training-batch-size 16 \
  --learning-rate 1e-4 \
  --log-dir 'results/models/downstream/ds002105' \
  --log-every-n-steps 1000 \
  --soft-prompt False \
  --soft-prompt-transformer False \
  --soft-prompt-vae True \
  --soft-prompt-lstm False \
  --is-prompt-residual False \
  --is-prompt-norm False \
  --freeze-embedder False \
  --freeze-decoder False \
  --freeze-cls-embed False \
  --shadow-module False \
  --mask-bold True \
  --vae-para 0.01 \
  --loss-hpara1 1.0 \
  --loss-hpara2 0.5 
